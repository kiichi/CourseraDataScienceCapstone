---
title: "Capstone Project Milestone Report"
author: "Kiichi Takeuchi"
date: "March 25, 2015"
output:
  html_document:
    theme: united
    toc: yes
  pdf_document:
    toc: yes
---

#Introduction


#Processing Data

First, I downloaded data from coursera website and unzipped it.

##Download and Unzip Data Files

```{r,cache=TRUE}
if (!file.exists("Coursera-SwiftKey.zip")){
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
                destfile = "Coursera-SwiftKey.zip", method="curl")
  unzip("Coursera-SwiftKey.zip")
}
```

After extracting data, I took profile of each files; such as the number of lines and words. See summaries below.

##Basic Sumamries of Files

In this section, I would like to present my explorartly analysis. Before jumping into other Natural Language Processing Libraries such as NLP or tm package, I used simple regex to split tokens.

```{r,echo=FALSE,cache=TRUE}
library(stringr)
library(xtable)
showFileSummary<-function(path){
  lines<-readLines(path, skipNul=TRUE, encoding = "UTF-8")
  tokens<-gregexpr("\\W+", lines)
  num_lines<-length(lines)  
  num_words<-sum(sapply(tokens, length) + 1)
  sm_words<-summary(sapply(tokens, length))
  sm_chars<-summary(sapply(lines, str_length))  
  print(xtable(data.frame(Lines=num_lines,TotalWords=num_words,
               SizeMB=file.info(path)$size/(1024^2))),type='html')  
  
  print(xtable(data.frame(MinWord=sm_words[[1]],MedianWord=sm_words[[3]],
               MeanWord=sm_words[[4]],MaxWord=sm_words[[6]],
               MinCh=sm_chars[[1]],MedianCh=sm_chars[[3]],
               MeanCh=sm_chars[[4]],MaxCh=sm_chars[[6]])),type='html')  
  rm(lines) #free up the memory
  rm(tokens)  
}
```

###Blogs
```{r,echo=FALSE,results='asis'}
showFileSummary('final/en_US/en_US.blogs.txt')
```

###News
```{r,echo=FALSE,results='asis'}
showFileSummary('final/en_US/en_US.news.txt')
```
###Twitter
```{r,echo=FALSE,results='asis'}
showFileSummary('final/en_US/en_US.twitter.txt')
```



##Profanity Words

In order to filter profanity words, I selected a list from [Front Game Media's Blog Article](http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/) because of the large number of words. The profainity file has to be cleaned too because contains website information and comma within the cell. The code below extract the list as character vector.

```{r,cache=TRUE}
if (!file.exists("Terms-to-Block.csv")){
  download.file("http://www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv", 
                destfile = "Terms-to-Block.csv", method="curl")  
}
profanity<-gsub(",","",read.csv("Terms-to-Block.csv")[-(1:3),2])
```

Here is the number of profanity words

```{r}
length(profanity)[[1]]
```

#Cleaning and Training Set

```{r,cache=TRUE}
createTrainingSet<-function(src_path,target_path,filter_words,ratio){
  lines<-readLines(src_path, skipNul=TRUE, encoding = "UTF-8")
  total<-length(lines)
  filter_regex<-paste(filter_words,collapse="|")
  lines<-sample(lines,total*ratio)
  lines<-gsub(filter_regex,"",lines)
  write(lines,file=target_path,sep="\t")
  rm(lines)
}
profanity<-gsub(",","",read.csv("Terms-to-Block.csv")[-(1:3),2])

# Sample Set: 5%
if (!file.exists("data/blog.csv")){
  createTrainingSet('final/en_US/en_US.blogs.txt','data/blog.csv',profanity,0.05)
  createTrainingSet('final/en_US/en_US.news.txt','data/news.csv',profanity,0.05)
  createTrainingSet('final/en_US/en_US.twitter.txt','data/twitter.csv',profanity,0.05)
  
  # Second Sample Set: 0.5%
  # Create Small Set for Wordcloud in Interesting Finding Section
  createTrainingSet('final/en_US/en_US.blogs.txt','data/blog_sm.csv',profanity,0.005)
  createTrainingSet('final/en_US/en_US.news.txt','data/news_sm.csv',profanity,0.005)
  createTrainingSet('final/en_US/en_US.twitter.txt','data/twitter_sm.csv',profanity,0.005)
}
```


#Features of Data




#Interesting Findings

```{r,cache=TRUE}
library(tm)
library(wordcloud)

generateWordCloud<-function(path){
  cps <- Corpus(DataframeSource(read.csv(path,sep='\t'))) 
  cps <- tm_map(cps, removePunctuation)
  cps <- tm_map(cps, function(x) removeWords(x, stopwords("english")))
  png(paste(path,".png",sep=""), width=400,height=300)
  options(mc.cores=1)  
  wordcloud(cps, scale=c(5,0.5), max.words=100, random.order=FALSE, 
            rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, 'Dark2'))
  dev.off()
}

generateWordCloud('data/blog_sm.csv')
generateWordCloud('data/news_sm.csv')
generateWordCloud('data/twitter_sm.csv')
```

[Blog](./images/blog_sm.csv.png)
https://raw.githubusercontent.com/kiichi/CourseraDataScienceCapstone/master/images/blog_sm.csv.png

[News](./images/news_sm.csv.png)
https://raw.githubusercontent.com/kiichi/CourseraDataScienceCapstone/master/images/news_sm.csv.png

[Twitter](./images/twitter_sm.csv.png)
https://raw.githubusercontent.com/kiichi/CourseraDataScienceCapstone/master/images/twitter_sm.csv.png

#Future Plans









