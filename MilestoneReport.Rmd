---
title: "Capstone Project Milestone Report"
author: "Kiichi Takeuchi"
date: "March 25, 2015"
output:
  html_document:
    theme: united
    toc: yes
  pdf_document:
    toc: yes
---

#Introduction

This is a milestone report of Coursera Data Science Course Capstone Project. The goal of this project is to read training data set in order to predict suggested words based on previous input.

In this report, I wil demonstrate that training data from swiftkey will be downloaded and preprocessed for later usage. Brief summaries of each files, exploratory analysis and future implementation plans will be presented in this report.


#Preparing Data

First, I downloaded data from coursera website and unzipped it.

##Download and Unzip Data Files

```{r,cache=TRUE}
if (!file.exists("Coursera-SwiftKey.zip")){
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
                destfile = "Coursera-SwiftKey.zip", method="curl")
  unzip("Coursera-SwiftKey.zip")
}
```

After extracting data, I took profile of each files; such as the number of lines and words. See summaries below.

#Basic Sumamries of Files

In this section, I would like to present my explorartly analysis. Before jumping into other Natural Language Processing Libraries such as RWeka or tm package, I used simple regex function to split tokens.

```{r,echo=FALSE,cache=TRUE}
library(stringr)
library(xtable)
showFileSummary<-function(path){
  lines<-readLines(path, skipNul=TRUE, encoding = "UTF-8")
  tokens<-gregexpr("\\W+", lines)
  num_lines<-length(lines)  
  num_words<-sum(sapply(tokens, length) + 1)
  sm_words<-summary(sapply(tokens, length))
  sm_chars<-summary(sapply(lines, str_length))  
  print(xtable(data.frame(Lines=num_lines,TotalWords=num_words,
               SizeMB=file.info(path)$size/(1024^2))),type='html')  
  
  print(xtable(data.frame(MinWord=sm_words[[1]],MedianWord=sm_words[[3]],
               MeanWord=sm_words[[4]],MaxWord=sm_words[[6]],
               MinCh=sm_chars[[1]],MedianCh=sm_chars[[3]],
               MeanCh=sm_chars[[4]],MaxCh=sm_chars[[6]])),type='html')  
  rm(lines) #free up the memory
  rm(tokens)  
}
```

##Blogs
```{r,echo=FALSE,results='asis',cache=TRUE}
showFileSummary('final/en_US/en_US.blogs.txt')
```

##News
```{r,echo=FALSE,results='asis',cache=TRUE}
showFileSummary('final/en_US/en_US.news.txt')
```
##Twitter
```{r,echo=FALSE,results='asis',cache=TRUE}
showFileSummary('final/en_US/en_US.twitter.txt')
```

##Profanity Words

In order to filter profanity words, I selected a list from [Front Game Media's Blog Article](http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/) because of the large number of words. The profainity file has to be cleaned too because contains website information and comma within the cell. The code below extract the list as character vector.

```{r,cache=TRUE}
if (!file.exists("Terms-to-Block.csv")){
  download.file("http://www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv", 
                destfile = "Terms-to-Block.csv", method="curl")  
}
profanity<-gsub(",","",read.csv("Terms-to-Block.csv")[-(1:3),2])
```

Here is the number of profanity words

```{r,echo=FALSE}
length(profanity)[[1]]
```

#Cleaning and Sampling Dataset

In this section, I am running script to generate sample / training set from the original data. The function, createTrainingSet generates 5% and 0.5% of data for later usage in this report.

```{r,cache=TRUE}
createTrainingSet<-function(src_path,target_path,filter_words,ratio){
  lines<-readLines(src_path, skipNul=TRUE, encoding = "UTF-8")
  total<-length(lines)
  filter_regex<-paste(filter_words,collapse="|")
  lines<-sample(lines,total*ratio)
  lines<-gsub(filter_regex,"",lines)
  write(lines,file=target_path,sep="\t")
  rm(lines)
}
profanity<-gsub(",","",read.csv("Terms-to-Block.csv")[-(1:3),2])

# Sample Set: 5%
if (!file.exists("data/blog.csv")){
  createTrainingSet('final/en_US/en_US.blogs.txt','data/blog.csv',profanity,0.05)
  createTrainingSet('final/en_US/en_US.news.txt','data/news.csv',profanity,0.05)
  createTrainingSet('final/en_US/en_US.twitter.txt','data/twitter.csv',profanity,0.05)
  
  # Second Sample Set: 0.5%
  # Create Small Set for Wordcloud in Interesting Finding Section
  createTrainingSet('final/en_US/en_US.blogs.txt','data/blog_sm.csv',profanity,0.005)
  createTrainingSet('final/en_US/en_US.news.txt','data/news_sm.csv',profanity,0.005)
  createTrainingSet('final/en_US/en_US.twitter.txt','data/twitter_sm.csv',profanity,0.005)
}
```


#Features of Data

I obtained clean and handy sample datasets for blog, news and twitter. These are basic tokeniers for unigram, bigram and trigram so that we can build corpus for each dataset. 

```{r,cache=TRUE}
library(tm)
library(RWeka)

options(mc.cores=1)
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
```

Next, let's build a function to read data files and generate histogram for each n-gram. 

After displaying histogram, Corpus object would be built and Text Document Matrix of Trigram is shown below.

Note: TermDocumentMatrix error in Rmd generator. I'm pasting the same result from my console.

```{r,cache=TRUE,echo=FALSE,message=FALSE,warning=FALSE}
library(ggplot2)
library(gridExtra)
generateHist<-function(path,tokenizer){  
  tokens<-tokenizer(readLines(path))
  freq_tbl<-head(sort(table(data.frame(tokens)),decreasing=T),20)
  freq_df<-data.frame(token=rownames(freq_tbl),freq=as.vector(freq_tbl))
  freq_df$token<-factor(freq_df$token,levels=freq_df[order(freq_df$freq),"token"])
  qplot(token,freq,data=freq_df,geom="bar",stat="identity",fill=freq) + coord_flip() + theme(legend.position="none")
}

showTDM<-function(path){
  cps <- Corpus(DataframeSource(read.csv(path,sep='\t'))) 
  cps <- tm_map(cps, removePunctuation)
  cps <- tm_map(cps, function(x) removeWords(x, stopwords("english")))
  TermDocumentMatrix(cps, control = list(tokenize = TrigramTokenizer))
}
```

##Blog
```{r,cache=TRUE,echo=FALSE}
path<-'data/blog_sm.csv'
grid.arrange(generateHist(path,UnigramTokenizer),
             generateHist(path,BigramTokenizer),
             generateHist(path,TrigramTokenizer),
             ncol=3)

#showTDM(path) 
```
Term Document Matrix
```
Non-/sparse entries: 103264/189682686
Sparsity           : 100%
Maximal term length: 143
```


##News
```{r,cache=TRUE,echo=FALSE}
path<-'data/news_sm.csv'
grid.arrange(generateHist(path,UnigramTokenizer),
             generateHist(path,BigramTokenizer),
             generateHist(path,TrigramTokenizer),
             ncol=3)
#showTDM(path)
```

Term Document Matrix
```
Non-/sparse entries: 101329/221472110
Sparsity           : 100%
Maximal term length: 54
```


##Twitter
```{r,cache=TRUE,echo=FALSE}
path<-'data/twitter_sm.csv'
grid.arrange(generateHist(path,UnigramTokenizer),
             generateHist(path,BigramTokenizer),
             generateHist(path,TrigramTokenizer),
             ncol=3)
#showTDM(path)
```

Term Document Matrix
```
Non-/sparse entries: 86865/444600595
Sparsity           : 100%
Maximal term length: 70
```


#Interesting Findings

Wordcoloud is a visualization technique to emphasize the font size of frequent occuring words. This is another intuitive way to interpret how words are used in the context.

As you see below, news repeateadly used word "said" while twitter scatteres wide range of vocavularies. Especially, I see positive emotional words, such as "good", "love" and "like".

```{r,cache=TRUE,echo=FALSE,message=FALSE,warning=FALSE}
library(tm)
library(wordcloud)

generateWordCloud<-function(path){
  #cps <- Corpus(VectorSource(as.vector(read.csv(path,sep='\t'))))  
  cps <- Corpus(DataframeSource(read.csv(path,sep='\t'))) 
  cps <- tm_map(cps, removePunctuation)
  cps <- tm_map(cps, function(x) removeWords(x, stopwords("english")))
  #png(paste(path,".png",sep=""), width=400,height=300)
  options(mc.cores=1)
  #BuGn or Dark2
  wordcloud(cps, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, 'Dark2'))
  #dev.off()
}
```

##Blog

```{r,cache=TRUE,echo=FALSE,message=FALSE,warning=FALSE}
generateWordCloud('data/blog_sm.csv')
```

##News

```{r,cache=TRUE,echo=FALSE,message=FALSE,warning=FALSE}
generateWordCloud('data/news_sm.csv')
```

##Twitter

```{r,cache=TRUE,echo=FALSE,message=FALSE,warning=FALSE}
generateWordCloud('data/twitter_sm.csv')
```


#Future Plans and Final Product

In production, 70% of the original dataset would be used. After building Corpus and Term Document Matrix from those files, following smoothing techniques will be examined:

1. Laplace Smoothing Simply Adding one (or normaized value between 0 and 1) so that we can avoid zero to estimate unknown combinations.
2. Removing Sparse Data (Rare occuring combination of words tend to be zero probability in training, so they will be removed)
3. Backoff models (e.g. if trigram is not found, backoff to bigram while we assume they will be appearing as same frequency)

The final product will be uploaded as Shyny App. I will provide user interface where the user can type in text, and prediction of words will be listed next to the textbox.






