---
title: "Capstone Project Milestone Report"
author: "Kiichi Takeuchi"
date: "March 25, 2015"
output:
  html_document:
    theme: united
    toc: yes
  pdf_document:
    toc: yes
---

#Introduction


#Processing Data


##Download and Unzip Data Files

```{r}
if (!file.exists("Coursera-SwiftKey.zip")){
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", 
                destfile = "Coursera-SwiftKey.zip", method="curl")
  unzip("Coursera-SwiftKey.zip")
}
```

##Basic Sumamries of Files

In this section, I would like to present my explorartly analysis. Before jumping into other Natural Language Processing Libraries such as NLP or tm package, I used simple regex to split tokens.

```{r,echo=FALSE}
library(stringr)
library(xtable)
showFileSummary<-function(path){
  lines<-readLines(path, skipNul=TRUE, encoding = "UTF-8")
  tokens<-gregexpr("\\W+", lines)
  num_lines<-length(lines)  
  num_words<-sum(sapply(tokens, length) + 1)
  sm_words<-summary(sapply(tokens, length))
  sm_chars<-summary(sapply(lines, str_length))  
  print(xtable(data.frame(Lines=num_lines,TotalWords=num_words,
               SizeMB=file.info(path)$size/(1024^2))),type='html')  
  
  print(xtable(data.frame(MinWord=sm_words[[1]],MedianWord=sm_words[[3]],
               MeanWord=sm_words[[4]],MaxWord=sm_words[[6]],
               MinCh=sm_chars[[1]],MedianCh=sm_chars[[3]],
               MeanCh=sm_chars[[4]],MaxCh=sm_chars[[6]])),type='html')  
  rm(lines) #free up the memory
  rm(tokens)  
}
```

###Blogs
```{r,echo=FALSE,results='asis'}
showFileSummary('final/en_US/en_US.blogs.txt')
```

###News
```{r,echo=FALSE,results='asis'}
showFileSummary('final/en_US/en_US.news.txt')
```
###Twitter
```{r,echo=FALSE,results='asis'}
showFileSummary('final/en_US/en_US.twitter.txt')
```



##Profanity Words

In order to filter profanity words, I selected a list from [Front Game Media's Blog Article](http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/) because of the large number of words. The profainity file has to be cleaned too because contains website information and comma within the cell. The code below extract the list as character vector.

```{r}
if (!file.exists("Terms-to-Block.csv")){
  download.file("http://www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv", destfile = "Terms-to-Block.csv", method="curl")  
}
profanity<-gsub(",","",read.csv("Terms-to-Block.csv")[-(1:3),2])
```

Here is the number of profanity words

```{r}
length(profanity)[[1]]
```

#Cleaning and Training Set

```{r}
createTrainingSet<-function(src_path,target_path,filter_words,ratio){
  lines<-readLines(src_path, skipNul=TRUE, encoding = "UTF-8")
  total<-length(lines)
  filter_regex<-paste(filter_words,collapse="|")
  lines<-sample(lines,total*ratio)
  lines<-gsub(filter_regex,"",lines)
  write(lines,file=target_path,sep="\t")
  rm(lines)
}
profanity<-gsub(",","",read.csv("Terms-to-Block.csv")[-(1:3),2])

# Training Set 5%
createTrainingSet('final/en_US/en_US.blogs.txt','data/blog.csv',profanity,0.05)
createTrainingSet('final/en_US/en_US.news.txt','data/news.csv',profanity,0.05)
createTrainingSet('final/en_US/en_US.twitter.txt','data/twitter.csv',profanity,0.05)

# Create Small Set for Wordcloud
createTrainingSet('final/en_US/en_US.blogs.txt','data/blog_sm.csv',profanity,0.005)
createTrainingSet('final/en_US/en_US.news.txt','data/news_sm.csv',profanity,0.005)
createTrainingSet('final/en_US/en_US.twitter.txt','data/twitter_sm.csv',profanity,0.005)
```


#Features of Data




#Interesting Findings

#Future Plans









